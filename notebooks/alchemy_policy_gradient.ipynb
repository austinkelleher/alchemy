{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alchemy_policy_gradient.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "peDbk6OfDhYn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone -b experimental https://git@github.com/wenkesj/alchemy.git ~/alchemy\n",
        "!(cd ~/alchemy; pip install .)\n",
        "!git clone https://github.com/openai/gym.git\n",
        "!(cd ~/gym; pip install .)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mO1jLMikDvak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "NbLIFmJgDjPx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import alchemy as ay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JevQeQ9pDtew",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ]
    },
    {
      "metadata": {
        "id": "_MRfFgCEDWJ7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "244be9a6-e621-4537-9ea1-dc701705a87d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525715192277,
          "user_tz": 240,
          "elapsed": 220,
          "user": {
            "displayName": "Sam Wenke",
            "photoUrl": "//lh6.googleusercontent.com/-f8Ky_WO2HTs/AAAAAAAAAAI/AAAAAAAAAG8/Q5eXh5dH1rg/s50-c-k-no/photo.jpg",
            "userId": "108962687437084869445"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "action_value_shape = [2]\n",
        "action_value_dtype = tf.float32\n",
        "stream = ay.rl.SimpleReplayStream.from_gym_env(\n",
        "    env, action_value_shape, action_value_dtype)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
            "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "MjWZIE73DxWS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Graph"
      ]
    },
    {
      "metadata": {
        "id": "Z_ZPRt61D63S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### `ay.rl.ReplayDataset`"
      ]
    },
    {
      "metadata": {
        "id": "vLM8kcZoDsDg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "hparams = tf.contrib.training.HParams(\n",
        "    learning_rate=1e-3,\n",
        "    initial_exploration=.5,\n",
        "    discount=.99,\n",
        "    exploration_decay_steps=2000,\n",
        "    exploration_decay_rate=.99,\n",
        "    max_sequence_length=200,\n",
        "    num_episodes=2000,\n",
        "    report_episode=50)\n",
        "\n",
        "\n",
        "replay_dataset = ay.rl.ReplayDataset(\n",
        "    stream, max_sequence_length=hparams.max_sequence_length)\n",
        "replay_op = replay_dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "\"\"\"\n",
        "TODO(wenkesj): Add this to `alchemy`, as a sort-of \"boostrapping\".\n",
        "\n",
        ">>> replay_op_with_advantage = ay.rl.add_advantage(replay_op)\n",
        "\"\"\"\n",
        "\n",
        "sequence_length = tf.squeeze(replay_op.sequence_length, 0)\n",
        "discounted_reward_op = ay.rl.discount_rewards(\n",
        "    replay_op.reward, \n",
        "    sequence_length=sequence_length,\n",
        "    max_sequence_length=hparams.max_sequence_length, \n",
        "    discount=hparams.discount)\n",
        "\n",
        "baseline_op = tf.cumsum(discounted_reward_op, reverse=False) / tf.cast(\n",
        "    sequence_length, discounted_reward_op.dtype)\n",
        "advantage_op = discounted_reward_op - baseline_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dE5gg8YZECN9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "PaMQUShBD_VI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def cart_pole_model_fn(state):\n",
        "  hidden = tf.layers.dense(state, 16, activation=tf.nn.relu, use_bias=False)\n",
        "  logits = tf.layers.dense(state, 2, use_bias=False)\n",
        "  return logits\n",
        "\n",
        "def cart_pole_policy_fn(action_values, exploration, deterministic):\n",
        "  shape = tf.shape(action_values)\n",
        "  deterministic_action = lambda: tf.argmax(\n",
        "      action_values, axis=-1, output_type=stream.action_dtype)\n",
        "  return tf.cond(\n",
        "      deterministic, \n",
        "      deterministic_action, \n",
        "      lambda: tf.cond(\n",
        "          exploration < tf.random_uniform([]), \n",
        "          deterministic_action,\n",
        "          lambda: tf.argmax(tf.random_uniform(shape), axis=-1, output_type=stream.action_dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrosC_nphISK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the necessary placeholders"
      ]
    },
    {
      "metadata": {
        "id": "SBl_2IEEceSu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "state_ph = tf.placeholder(\n",
        "    stream.state_dtype, \n",
        "    [None] + list(stream.state_shape),\n",
        "    name='state') # previous states\n",
        "action_ph = tf.placeholder(\n",
        "    stream.action_dtype, \n",
        "    [None] + list(stream.action_shape), \n",
        "    name='action') # previous actions\n",
        "advantage_ph = tf.placeholder(\n",
        "    stream.reward_dtype,\n",
        "    [], name='advantage')\n",
        "sequence_length_ph = tf.placeholder(\n",
        "    tf.int32, [], name='sequence_length')\n",
        "deterministic_ph = tf.placeholder(\n",
        "    tf.bool, [],\n",
        "    name='deterministic') # deterministic|epsilon actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cn-uXDIUhCqs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Epsilon-Greedy Policy"
      ]
    },
    {
      "metadata": {
        "id": "2pgwkr6wGusq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "exploration_op = tf.train.exponential_decay(\n",
        "    hparams.initial_exploration, \n",
        "    global_step, \n",
        "    hparams.exploration_decay_steps, \n",
        "    hparams.exploration_decay_rate)\n",
        "\n",
        "action_values_op = cart_pole_model_fn(state_ph)\n",
        "action_op = cart_pole_policy_fn(action_values_op, exploration_op, deterministic_ph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gahuAVFX1QGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ]
    },
    {
      "metadata": {
        "id": "4rca9Knt1Rdm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "loss_op = tf.losses.sparse_softmax_cross_entropy(\n",
        "    action_ph, action_values_op, reduction=tf.losses.Reduction.NONE)\n",
        "loss_op = tf.reduce_sum(\n",
        "    tf.reduce_sum(loss_op, axis=-1) * advantage_ph) / tf.cast(\n",
        "        sequence_length_ph, loss_op.dtype)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(\n",
        "    learning_rate=hparams.learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i8FijXWDD2Cn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "metadata": {
        "id": "lshdnjsCYSez",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def run_episode(env, sess):\n",
        "  experiences = []\n",
        "  next_state = env.reset()\n",
        "  while True:\n",
        "    state = next_state\n",
        "    action_values, action = sess.run(\n",
        "        (action_values_op, action_op), \n",
        "        feed_dict={\n",
        "            state_ph: [state], \n",
        "            deterministic_ph: False\n",
        "        })\n",
        "\n",
        "    next_state, reward, terminal, _ = env.step(np.squeeze(action, 0))\n",
        "    experiences.append(ay.rl.Experience(\n",
        "        state, next_state, action, action_values, reward, terminal))\n",
        "    if terminal:\n",
        "      break\n",
        "  return experiences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x39FF0ceDrbU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1085
        },
        "outputId": "0d811d3b-1766-45e6-882e-c54712ec2a3a",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525715303719,
          "user_tz": 240,
          "elapsed": 109123,
          "user": {
            "displayName": "Sam Wenke",
            "photoUrl": "//lh6.googleusercontent.com/-f8Ky_WO2HTs/AAAAAAAAAAI/AAAAAAAAAG8/Q5eXh5dH1rg/s50-c-k-no/photo.jpg",
            "userId": "108962687437084869445"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for episode in range(hparams.num_episodes):\n",
        "    experiences = run_episode(env, sess)\n",
        "    stream.write(\n",
        "        ay.rl.Replay(\n",
        "            *zip(*experiences), sequence_length=len(experiences)))\n",
        "\n",
        "    \n",
        "    replay, advantage = sess.run((replay_op, advantage_op))\n",
        "    for t in range(replay.sequence_length[0]):\n",
        "      _, loss = sess.run(\n",
        "          (train_op, loss_op), \n",
        "          feed_dict={\n",
        "              state_ph: [replay.state[t]],\n",
        "              action_ph: [replay.action[t]], \n",
        "              advantage_ph: advantage[t],\n",
        "              deterministic_ph: True,\n",
        "              sequence_length_ph: replay.sequence_length[0],\n",
        "          })\n",
        "\n",
        "    if (episode + 1) % hparams.report_episode == 0:\n",
        "      experiences = run_episode(env, sess)\n",
        "      replay = ay.rl.Replay(\n",
        "            *zip(*experiences), sequence_length=len(experiences))\n",
        "      print('reward = {}, episode = {}'.format(sum(replay.reward), episode + 1))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward = 99.0, episode = 50\n",
            "reward = 40.0, episode = 100\n",
            "reward = 43.0, episode = 150\n",
            "reward = 124.0, episode = 200\n",
            "reward = 158.0, episode = 250\n",
            "reward = 20.0, episode = 300\n",
            "reward = 112.0, episode = 350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5c484df15e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m               \u001b[0madvantage_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mdeterministic_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m               \u001b[0msequence_length_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m           })\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}