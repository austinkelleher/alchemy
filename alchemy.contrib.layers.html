

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>alchemy.contrib.layers package &mdash; alchemy 1.0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="alchemy.contrib.rl package" href="alchemy.contrib.rl.html" />
    <link rel="prev" title="alchemy.contrib.data package" href="alchemy.contrib.data.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> alchemy
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="alchemy.contrib.data.html">Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Abstract Layers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-alchemy.contrib.layers.autoencoder">alchemy.contrib.layers.autoencoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-alchemy.contrib.layers.lateral_impl">alchemy.contrib.layers.lateral_impl module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-alchemy.contrib.layers.wta_impl">alchemy.contrib.layers.wta_impl module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-alchemy.contrib.layers">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="alchemy.contrib.rl.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="alchemy.contrib.rnn.html">Recurrent Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="alchemy.contrib.train.html">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="alchemy.multiprocessing.html">Multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="alchemy.utils.html">Utility</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">alchemy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>alchemy.contrib.layers package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/alchemy.contrib.layers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="alchemy-contrib-layers-package">
<h1>alchemy.contrib.layers package<a class="headerlink" href="#alchemy-contrib-layers-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-alchemy.contrib.layers.autoencoder">
<span id="alchemy-contrib-layers-autoencoder-module"></span><h2>alchemy.contrib.layers.autoencoder module<a class="headerlink" href="#module-alchemy.contrib.layers.autoencoder" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="alchemy.contrib.layers.autoencoder.conv2d_decoder">
<code class="descclassname">alchemy.contrib.layers.autoencoder.</code><code class="descname">conv2d_decoder</code><span class="sig-paren">(</span><em>inputs</em>, <em>encoder</em>, <em>shapes</em>, <em>strides</em>, <em>scope=None</em>, <em>activation=None</em>, <em>weight_sharing=False</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.autoencoder.conv2d_decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="alchemy.contrib.layers.autoencoder.conv2d_encoder">
<code class="descclassname">alchemy.contrib.layers.autoencoder.</code><code class="descname">conv2d_encoder</code><span class="sig-paren">(</span><em>inputs</em>, <em>filters</em>, <em>kernel_sizes</em>, <em>strides</em>, <em>scope=None</em>, <em>activation=None</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.autoencoder.conv2d_encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="alchemy.contrib.layers.autoencoder.conv2d_rnn_encoder">
<code class="descclassname">alchemy.contrib.layers.autoencoder.</code><code class="descname">conv2d_rnn_encoder</code><span class="sig-paren">(</span><em>inputs</em>, <em>input_shape</em>, <em>filters</em>, <em>kernel_sizes</em>, <em>strides</em>, <em>activation</em>, <em>latent_hidden_sizes</em>, <em>latent_hidden_activation</em>, <em>rnn_hidden_sizes</em>, <em>rnn_cell_fn</em>, <em>scope</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.autoencoder.conv2d_rnn_encoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-alchemy.contrib.layers.lateral_impl">
<span id="alchemy-contrib-layers-lateral-impl-module"></span><h2>alchemy.contrib.layers.lateral_impl module<a class="headerlink" href="#module-alchemy.contrib.layers.lateral_impl" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="alchemy.contrib.layers.lateral_impl.Lateral">
<em class="property">class </em><code class="descclassname">alchemy.contrib.layers.lateral_impl.</code><code class="descname">Lateral</code><span class="sig-paren">(</span><em>allow_self_connections=False</em>, <em>activation=None</em>, <em>use_bias=True</em>, <em>kernel_initializer=None</em>, <em>bias_initializer=&lt;tensorflow.python.ops.init_ops.Zeros object&gt;</em>, <em>kernel_regularizer=None</em>, <em>bias_regularizer=None</em>, <em>activity_regularizer=None</em>, <em>kernel_constraint=None</em>, <em>bias_constraint=None</em>, <em>kernel_trainable=True</em>, <em>trainable=True</em>, <em>name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.lateral_impl.Lateral" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.layers.base.Layer</span></code></p>
<p>Lateral-connected layer class.
This layer adds lateral connections.
:param allow_self_connections: Boolean <cite>Tensor</cite>. Scalar representing if the</p>
<blockquote>
<div>lateral connections should have self-connections, (e.g. kernel diagonal
is non-zero). Default is <cite>False</cite>.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>activation</strong> – Activation function (callable). Set it to None to maintain a
linear activation.</li>
<li><strong>use_bias</strong> – Boolean, whether the layer uses a bias.</li>
<li><strong>kernel_initializer</strong> – Initializer function for the weight matrix.
If <cite>None</cite> (default), weights are initialized using the default
initializer used by <cite>tf.get_variable</cite>.</li>
<li><strong>bias_initializer</strong> – Initializer function for the bias.</li>
<li><strong>kernel_regularizer</strong> – Regularizer function for the weight matrix.</li>
<li><strong>bias_regularizer</strong> – Regularizer function for the bias.</li>
<li><strong>activity_regularizer</strong> – Regularizer function for the output.</li>
<li><strong>kernel_constraint</strong> – An optional projection function to be applied to the
kernel after being updated by an <cite>Optimizer</cite> (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training.</li>
<li><strong>bias_constraint</strong> – An optional projection function to be applied to the
bias after being updated by an <cite>Optimizer</cite>.</li>
<li><strong>kernel_trainable</strong> – Boolean, kernel should be added to the graph collection
<cite>GraphKeys.TRAINABLE_VARIABLES</cite> (see <cite>tf.Variable</cite>).</li>
<li><strong>trainable</strong> – Boolean, if <cite>True</cite> also add variables to the graph collection
<cite>GraphKeys.TRAINABLE_VARIABLES</cite> (see <cite>tf.Variable</cite>).</li>
<li><strong>name</strong> – String, the name of the layer. Layers with the same name will
share weights, but to avoid mistakes we require reuse=True in such cases.</li>
<li><strong>reuse</strong> – Boolean, whether to reuse the weights of a previous layer
by the same name.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Properties:</dt>
<dd><dl class="first docutils">
<dt>allow_self_connections: Boolean <cite>Tensor</cite>. Scalar representing if the</dt>
<dd>lateral connections should have self-connections.</dd>
</dl>
<p class="last">activation: Activation function (callable).
use_bias: Boolean, whether the layer uses a bias.
kernel_initializer: Initializer instance (or name) for the kernel matrix.
bias_initializer: Initializer instance (or name) for the bias.
kernel_regularizer: Regularizer instance for the kernel matrix (callable)
bias_regularizer: Regularizer instance for the bias (callable).
activity_regularizer: Regularizer instance for the output (callable)
kernel_constraint: Constraint function for the kernel matrix.
bias_constraint: Constraint function for the bias.
kernel: Weight matrix (TensorFlow variable or tensor).
bias: Bias vector, if applicable (TensorFlow variable or tensor).</p>
</dd>
</dl>
<dl class="method">
<dt id="alchemy.contrib.layers.lateral_impl.Lateral.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.lateral_impl.Lateral.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer.</p>
</dd></dl>

<dl class="method">
<dt id="alchemy.contrib.layers.lateral_impl.Lateral.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.lateral_impl.Lateral.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic of the layer lives here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>inputs</strong> – input tensor(s).</li>
<li><strong>**kwargs</strong> – additional keyword arguments.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Output tensor(s).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="alchemy.contrib.layers.lateral_impl.Lateral.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.lateral_impl.Lateral.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer given the input shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>input_shape</strong> – A (possibly nested tuple of) <cite>TensorShape</cite>.  It need not
be fully defined (e.g. the batch size may be unknown).</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A (possibly nested tuple of) <cite>TensorShape</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code> – if <cite>input_shape</cite> is not a (possibly nested tuple of)
<cite>TensorShape</cite>.</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code> – if <cite>input_shape</cite> is incomplete or is incompatible with the
the layer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="alchemy.contrib.layers.lateral_impl.lateral">
<code class="descclassname">alchemy.contrib.layers.lateral_impl.</code><code class="descname">lateral</code><span class="sig-paren">(</span><em>inputs</em>, <em>allow_self_connections=False</em>, <em>activation=None</em>, <em>use_bias=True</em>, <em>kernel_initializer=None</em>, <em>bias_initializer=&lt;tensorflow.python.ops.init_ops.Zeros object&gt;</em>, <em>kernel_regularizer=None</em>, <em>bias_regularizer=None</em>, <em>activity_regularizer=None</em>, <em>kernel_constraint=None</em>, <em>bias_constraint=None</em>, <em>kernel_trainable=True</em>, <em>trainable=True</em>, <em>name=None</em>, <em>reuse=None</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.lateral_impl.lateral" title="Permalink to this definition">¶</a></dt>
<dd><p>Functional interface for the lateral-connected layer.
This layer adds lateral connections.
:param inputs: Tensor input.
:param allow_self_connections: Boolean <cite>Tensor</cite>. Scalar representing if the</p>
<blockquote>
<div>lateral connections should have self-connections, (e.g. kernel diagonal
is non-zero). Default is <cite>False</cite>.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>activation</strong> – Activation function (callable). Set it to None to maintain a
linear activation.</li>
<li><strong>use_bias</strong> – Boolean, whether the layer uses a bias.</li>
<li><strong>kernel_initializer</strong> – Initializer function for the weight matrix.
If <cite>None</cite> (default), weights are initialized using the default
initializer used by <cite>tf.get_variable</cite>.</li>
<li><strong>bias_initializer</strong> – Initializer function for the bias.</li>
<li><strong>kernel_regularizer</strong> – Regularizer function for the weight matrix.</li>
<li><strong>bias_regularizer</strong> – Regularizer function for the bias.</li>
<li><strong>activity_regularizer</strong> – Regularizer function for the output.</li>
<li><strong>kernel_constraint</strong> – An optional projection function to be applied to the
kernel after being updated by an <cite>Optimizer</cite> (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training.</li>
<li><strong>bias_constraint</strong> – An optional projection function to be applied to the
bias after being updated by an <cite>Optimizer</cite>.</li>
<li><strong>kernel_trainable</strong> – Boolean, kernel should be added to the graph collection
<cite>GraphKeys.TRAINABLE_VARIABLES</cite> (see <cite>tf.Variable</cite>).</li>
<li><strong>trainable</strong> – Boolean, if <cite>True</cite> also add variables to the graph collection
<cite>GraphKeys.TRAINABLE_VARIABLES</cite> (see <cite>tf.Variable</cite>).</li>
<li><strong>name</strong> – String, the name of the layer.</li>
<li><strong>reuse</strong> – Boolean, whether to reuse the weights of a previous layer
by the same name.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Output tensor the same shape as <cite>inputs</cite> except the last dimension is of
size <cite>units</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code> – if eager execution is enabled.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-alchemy.contrib.layers.wta_impl">
<span id="alchemy-contrib-layers-wta-impl-module"></span><h2>alchemy.contrib.layers.wta_impl module<a class="headerlink" href="#module-alchemy.contrib.layers.wta_impl" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="alchemy.contrib.layers.wta_impl.DWTA">
<em class="property">class </em><code class="descclassname">alchemy.contrib.layers.wta_impl.</code><code class="descname">DWTA</code><span class="sig-paren">(</span><em>name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.wta_impl.DWTA" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.layers.base.Layer</span></code></p>
<p>Applies d-WTA to the input.
Dropout consists in keeping top-k activations and zero-ing out the rest.
:param k: The top-k, between 0 and <cite>input_shape[-1]</cite>.
:param name: The name of the layer (string).</p>
<dl class="method">
<dt id="alchemy.contrib.layers.wta_impl.DWTA.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em>, <em>training=False</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.wta_impl.DWTA.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic of the layer lives here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>inputs</strong> – input tensor(s).</li>
<li><strong>**kwargs</strong> – additional keyword arguments.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Output tensor(s).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="alchemy.contrib.layers.wta_impl.DWTA.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.wta_impl.DWTA.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer given the input shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>input_shape</strong> – A (possibly nested tuple of) <cite>TensorShape</cite>.  It need not
be fully defined (e.g. the batch size may be unknown).</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A (possibly nested tuple of) <cite>TensorShape</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code> – if <cite>input_shape</cite> is not a (possibly nested tuple of)
<cite>TensorShape</cite>.</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code> – if <cite>input_shape</cite> is incomplete or is incompatible with the
the layer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="alchemy.contrib.layers.wta_impl.WTA">
<em class="property">class </em><code class="descclassname">alchemy.contrib.layers.wta_impl.</code><code class="descname">WTA</code><span class="sig-paren">(</span><em>k</em>, <em>name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.wta_impl.WTA" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.layers.base.Layer</span></code></p>
<p>Applies WTA to the input.
Dropout consists in keeping top-k activations and zero-ing out the rest.
:param k: The top-k, between 0 and <cite>input_shape[-1]</cite>.
:param name: The name of the layer (string).</p>
<dl class="method">
<dt id="alchemy.contrib.layers.wta_impl.WTA.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em>, <em>training=False</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.wta_impl.WTA.call" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic of the layer lives here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>inputs</strong> – input tensor(s).</li>
<li><strong>**kwargs</strong> – additional keyword arguments.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Output tensor(s).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="alchemy.contrib.layers.wta_impl.WTA.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.wta_impl.WTA.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer given the input shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>input_shape</strong> – A (possibly nested tuple of) <cite>TensorShape</cite>.  It need not
be fully defined (e.g. the batch size may be unknown).</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A (possibly nested tuple of) <cite>TensorShape</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code> – if <cite>input_shape</cite> is not a (possibly nested tuple of)
<cite>TensorShape</cite>.</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code> – if <cite>input_shape</cite> is incomplete or is incompatible with the
the layer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="alchemy.contrib.layers.wta_impl.d_wta">
<code class="descclassname">alchemy.contrib.layers.wta_impl.</code><code class="descname">d_wta</code><span class="sig-paren">(</span><em>inputs</em>, <em>name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.wta_impl.d_wta" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies d-WTA to the input.
Dropout consists in keeping top-k activations and zero-ing out the rest.
:param inputs: Tensor input.
:param k: The top-k, between 0 and <cite>input_shape[-1]</cite>.
:param name: The name of the layer (string).</p>
</dd></dl>

<dl class="function">
<dt id="alchemy.contrib.layers.wta_impl.wta">
<code class="descclassname">alchemy.contrib.layers.wta_impl.</code><code class="descname">wta</code><span class="sig-paren">(</span><em>inputs</em>, <em>k</em>, <em>name=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#alchemy.contrib.layers.wta_impl.wta" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies WTA to the input.
Dropout consists in keeping top-k activations and zero-ing out the rest.
:param inputs: Tensor input.
:param k: The top-k, between 0 and <cite>input_shape[-1]</cite>.
:param name: The name of the layer (string).</p>
</dd></dl>

</div>
<div class="section" id="module-alchemy.contrib.layers">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-alchemy.contrib.layers" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="alchemy.contrib.rl.html" class="btn btn-neutral float-right" title="alchemy.contrib.rl package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="alchemy.contrib.data.html" class="btn btn-neutral" title="alchemy.contrib.data package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Sam Wenke.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0.0',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>